{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743239fe-0937-4697-a454-968c917a8c85",
   "metadata": {},
   "source": [
    "# Ray Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdcfc5f-dc78-4f60-959c-b21bf21c899a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pieces from codeflare-sdk\n",
    "from codeflare_sdk.cluster.cluster import Cluster, ClusterConfiguration\n",
    "from codeflare_sdk.cluster.auth import TokenAuthentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef612c3a-f78c-44ed-b6e7-f8c9384849b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create authentication object for user permissions\n",
    "# IF unused, SDK will automatically check for default kubeconfig, then in-cluster config\n",
    "# KubeConfigFileAuthentication can also be used to specify kubeconfig path manually\n",
    "auth = TokenAuthentication(\n",
    "    token = \"\",\n",
    "    server = \"\",\n",
    "    skip_tls=True\n",
    ")\n",
    "auth.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa686fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR USERNAME HERE\n",
    "NAMESPACE = !cat /var/run/secrets/kubernetes.io/serviceaccount/namespace\n",
    "NAMESPACE = NAMESPACE[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a32d09-6fcd-4536-94c7-d32d4e34e55b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and configure our cluster object (and appwrapper)\n",
    "cluster = Cluster(ClusterConfiguration(\n",
    "    name='raytest',\n",
    "    namespace=NAMESPACE,\n",
    "    num_workers=2,\n",
    "    min_cpus=4,\n",
    "    max_cpus=4,\n",
    "    min_memory=8,\n",
    "    max_memory=8,\n",
    "    num_gpus=0,\n",
    "    image=\"quay.io/project-codeflare/ray:latest-py39-cu118\",\n",
    "    instascale=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84784940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import update_yaml\n",
    "update_yaml.namespace_specific_yaml(NAMESPACE)\n",
    "!cp raytest.yaml ~/.codeflare/appwrapper/raytest.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d404e4-40bb-4787-815e-03984908e3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bring up the cluster\n",
    "cluster.up()\n",
    "cluster.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a643f-b62c-4005-b3d4-d9f038b9efba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aba392-3da3-407f-b88f-4474485dd854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray_dashboard_uri = cluster.cluster_dashboard_uri()\n",
    "ray_cluster_uri = cluster.cluster_uri()\n",
    "print(ray_dashboard_uri)\n",
    "print(ray_cluster_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d90e6-6068-488e-be4f-bed3528bbd64",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c5176c-14c1-4d5a-ba45-535dd94f1a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb69e6-5c81-4165-91f9-8a2d19594f51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data = pd.read_csv('data/card_transdata.csv')\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d431ab7-3849-41f4-b753-437235efc086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the input (X) and output (Y) data. \n",
    "# The only output data is whether it's fraudulent. All other fields are inputs to the model.\n",
    "\n",
    "X = Data.drop(columns = ['repeat_retailer','distance_from_home', 'fraud'])\n",
    "y = Data['fraud']\n",
    "\n",
    "# Split the data into training and testing sets so you have something to test the trained model with.\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, stratify = y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, shuffle = False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size = 0.2, stratify = y_train)\n",
    "\n",
    "# Scale the data to remove mean and have unit variance. The data will be between -1 and 1, which makes it a lot easier for the model to learn than random (and potentially large) values.\n",
    "# It is important to only fit the scaler to the training data, otherwise you are leaking information about the global distribution of variables (which is influenced by the test set) into the training set.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train.values)\n",
    "\n",
    "# Since the dataset is unbalanced (it has many more non-fraud transactions than fraudulent ones), set a class weight to weight the few fraudulent transactions higher than the many non-fraud transactions.\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes = np.unique(y_train),y = y_train)\n",
    "class_weights = {i : class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7c2ac-e491-414f-82ba-260e78a71ee7",
   "metadata": {},
   "source": [
    "# Get S3 info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c930799-d0c9-4b2b-87a9-20010441cbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from os import environ\n",
    "\n",
    "s3_endpoint_url = environ.get('AWS_S3_ENDPOINT')\n",
    "s3_access_key = environ.get('AWS_ACCESS_KEY_ID')\n",
    "s3_secret_key = environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "s3_bucket_name = environ.get('AWS_S3_BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a1575-6f80-4c8c-a413-6317ddc2aa10",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b090b-2a76-4cf3-98c9-ec81e6dc8b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d193b-16a3-4960-9c76-a2d7e7d9018b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#before proceeding make sure the cluster exists and the uri is not empty\n",
    "assert ray_cluster_uri, \"Ray cluster needs to be started and set before proceeding\"\n",
    "\n",
    "import ray\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "# reset the ray context in case there's already one. \n",
    "ray.shutdown()\n",
    "# establish connection to ray cluster\n",
    "\n",
    "#install additional libraries that will be required for model training\n",
    "# with open('requirements.txt') as f:\n",
    "#     requirements = f.read().splitlines()\n",
    "runtime_env = {\"pip\": [\"tensorflow==2.10\"]}\n",
    "\n",
    "# NOTE: This will work for in-cluster notebook servers (RHODS/ODH), but not for local machines\n",
    "# To see how to connect from your laptop, go to demo-notebooks/additional-demos/local_interactive.ipynb\n",
    "ray.init(address=ray_cluster_uri, runtime_env=runtime_env)\n",
    "\n",
    "print(\"Ray cluster is up and running: \", ray.is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b960c39-8e82-4047-b289-63b6f5169940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_fn():\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "    from ray import train\n",
    "    from ray.train import Checkpoint\n",
    "    from ray.train.tensorflow import TensorflowTrainer\n",
    "    from ray.train.tensorflow.keras import ReportCheckpointCallback\n",
    "    import tensorflow as tf\n",
    "    import tempfile\n",
    "    import os\n",
    "    \n",
    "    def build_model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, activation = 'relu', input_dim = len(X.columns)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(32))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(32))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation = 'sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def train_model(config: dict):\n",
    "        batch_size = config.get(\"batch_size\", 32)\n",
    "        batch_size = batch_size * train.get_context().get_world_size()\n",
    "        epochs = config.get(\"epochs\", 2)\n",
    "        \n",
    "        # Build the model\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        with strategy.scope():\n",
    "            model = build_model()\n",
    "            model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "            model.summary()\n",
    "\n",
    "        # Get the dataset\n",
    "        dataset = train.get_dataset_shard(\"train\")\n",
    "        tf_dataset = dataset.to_tf(\n",
    "            feature_columns=\"x\", label_columns=\"y\", batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        results = []\n",
    "        history = model.fit(\n",
    "            tf_dataset, \n",
    "            epochs=epochs,\n",
    "            callbacks=[ReportCheckpointCallback()]\n",
    "        )\n",
    "        results.append(history.history)\n",
    "        \n",
    "        # Push results to S3\n",
    "        s3 = boto3.client(\n",
    "            's3', endpoint_url=s3_endpoint_url,\n",
    "            aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key,\n",
    "        )\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            model_location = os.path.join(temp_checkpoint_dir, \"model.keras\")\n",
    "            model.save(model_location)\n",
    "            s3.upload_file(model_location, s3_bucket_name, 'model.keras')\n",
    "\n",
    "        return results\n",
    "    \n",
    "    # Run everything distributed\n",
    "    config = {\"batch_size\": 32, \"epochs\": 2}\n",
    "    reshaped_dataset = [{\"x\": X_train[i], \"y\":y_train.values[i]} for i in range(len(X_train))]\n",
    "    train_dataset = ray.data.from_items(reshaped_dataset)\n",
    "    scaling_config = ScalingConfig(num_workers=2, use_gpu=False)\n",
    "    \n",
    "    trainer = TensorflowTrainer(\n",
    "        train_loop_per_worker=train_model,\n",
    "        train_loop_config=config,\n",
    "        scaling_config=scaling_config,\n",
    "        datasets={\"train\": train_dataset},\n",
    "    )\n",
    "    result = trainer.fit()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f07e9-70d4-4896-b8f3-fdb13df793d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = ray.get(train_fn.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186b18c-ccb8-4e22-b440-5852e48059bc",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa5d1f7-ded5-453b-b752-d0a63727e14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install onnx tf2onnx tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ccb9c-cd33-4a54-8b9e-391069baf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bfabae-57db-4062-be8d-7d169dc37702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the file we trained\n",
    "s3 = boto3.client(\n",
    "            's3', endpoint_url=s3_endpoint_url,\n",
    "            aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key,\n",
    "        )\n",
    "s3.download_file(\n",
    "        s3_bucket_name, \"model.keras\", 'model.keras'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4211a0-2b3c-4f74-a50b-7c81fc797284",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model.keras\")\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model)\n",
    "os.makedirs(\"models/fraud\", exist_ok=True)\n",
    "onnx.save(model_proto, \"models/fraud/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90307cca-4255-4cde-83ce-3f7019dffa05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# And push it back up to S3\n",
    "s3.upload_file(\"models/fraud/model.onnx\", s3_bucket_name, 'model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904932a-89c1-476f-91b1-d3ae9ca50cd4",
   "metadata": {},
   "source": [
    "# Shut down cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17658a68-2681-4805-aa14-5455a55de320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91faac7-2e1c-41d7-a8c0-675165c20f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.logout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
