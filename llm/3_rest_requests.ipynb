{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73046ff",
   "metadata": {},
   "source": [
    "# REST Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7eb303-2ded-41b5-91db-8b8a8860d2ac",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93983d-1c27-4a3e-b9fd-c99112fa6d2a",
   "metadata": {},
   "source": [
    "### Install the client package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b543037-16ff-4eea-9664-94256db0b9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install caikit-nlp-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae826aec-1732-44d0-9aa0-be7538875f50",
   "metadata": {},
   "source": [
    "\n",
    "Change that following variable settings match your deployed model's *Inference endpoint*. for example: \n",
    "\n",
    "```\n",
    "infer_endpoint = \"https://flan-t5-small-caikit-predictor-userx-workshop.apps.clusterx.sandboxx.opentlc.com\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17b252-7827-4cae-adb0-f98c9d80bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"flan-t5-small-caikit\"\n",
    "infer_endpoint = \"https://flan-t5-small-caikit-predictor-userx-workshop.apps.<cluster>.com\"\n",
    "infer_url = f\"{infer_endpoint}/api/v1/task/text-generation\"\n",
    "str_infer_url = f\"{infer_endpoint}/api/v1/task/server-streaming-text-generation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d609e69c-9528-435c-a92a-7730f1ae91b2",
   "metadata": {},
   "source": [
    "## Using the client library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd61c47-8d82-4a60-98eb-d87d56a8590a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from caikit_nlp_client import HttpClient\n",
    "\n",
    "http_client = HttpClient(infer_endpoint, verify=False)\n",
    "\n",
    "text = http_client.generate_text(model_id, \"At what temperature does Nitrogen boil?\")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5835f05-7fe2-4d51-bed5-3aa9f2b380fe",
   "metadata": {},
   "source": [
    "## Python Request Function\n",
    "\n",
    "Build and submit the REST request. \n",
    "\n",
    "Note: You submit the data in the same format that you used for an ONNX inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1d001-ff99-414a-95d4-5729d5849298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def rest_request(data):\n",
    "    json_data = {\n",
    "        \"model_id\": model_id,\n",
    "        \"inputs\": data,\n",
    "    }\n",
    "\n",
    "    response = requests.post(infer_url, json=json_data, verify=False)\n",
    "    response_dict = response.json()\n",
    "    return response_dict[\"generated_text\"]\n",
    "\n",
    "\n",
    "def streaming_rest_request(data):\n",
    "    json_data = {\n",
    "        \"model_id\": model_id,\n",
    "        \"inputs\": data,\n",
    "    }\n",
    "\n",
    "    response = requests.post(infer_url, json=json_data, stream=True, verify=False)\n",
    "    response_tokens = []\n",
    "    for token in response.iter_lines():\n",
    "        # we could display each token to the user as it comes in, but for now, we'll just display at the end \n",
    "        decoded_token = token.decode(\"utf-8\")\n",
    "        response_tokens.append(decoded_token)\n",
    "\n",
    "    return response_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a68b67-b109-4a2f-b097-092f4a4d25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rest_request(\"At what temperature does Nitrogen boil?\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171bce3d-4a15-4d9b-a190-dd469c21c309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = streaming_rest_request(\"At what temperature does Nitrogen boil?\")\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
